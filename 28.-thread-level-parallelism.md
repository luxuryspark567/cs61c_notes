# 28. Thread Level Parallelism

## Improve Performance

1. <mark style="color:yellow;">Increase clock rate</mark>
   1. ﻿﻿Reached practical maximum for today's technology
   2. <mark style="color:yellow;">< 5GHz for general purpose computers</mark>
2. <mark style="color:yellow;">Lower CPI (cycles per instruction)</mark>
   1. SIMD, "instruction level parallelism"
3. <mark style="color:yellow;">Perform multiple tasks simultaneously</mark>
   1. Multiple CPUs, each executing different program
   2. ﻿﻿Tasks may be related
      1. E.g. each CPU performs part of a big matrix multiplication&#x20;
   3. or unrelated
      1. E.g. distribute different web hip requests over different computers
      2. E.g. run pptx (view lecture slides) and browser (youtube)\
         simultaneously

## Example: CPU with 2 Cores

The biggest challenge is memory sharing.

<figure><img src=".gitbook/assets/image (1).png" alt="" width="375"><figcaption></figcaption></figure>

## Multiprocessor Execution Model

Each processor (core) executes its own instructions

* Separate resources (not shared)
  * Datapath (PC, registers, ALU)
  * ﻿﻿Highest level caches (e.g., 1st and 2nd)
* ﻿﻿<mark style="color:yellow;">Shared resources</mark>
  * ﻿﻿<mark style="color:yellow;">Memory (DRAM)</mark>
  * ﻿﻿<mark style="color:yellow;">Often 3rd level cache</mark>
    * ﻿﻿Often on same silicon chip
    * ﻿﻿But not a requirement
* ﻿﻿Nomenclature
  * ﻿﻿"Multiprocessor Microprocessor"
  * ﻿﻿Multicore processor
    * ﻿﻿E.g., four core CPU (central processing unit)
    * ﻿﻿Executes four different instruction streams simultaneously
* Shared memory
  * ﻿﻿Each "core" has access to the entire memory in the processor
  * ﻿﻿<mark style="color:yellow;">Special hardware keeps caches consistent</mark>
  * ﻿﻿<mark style="color:yellow;">Advantages</mark>:
    * ﻿﻿<mark style="color:yellow;">Simplifies communication in program via shared variables</mark>
  * ﻿﻿<mark style="color:yellow;">Drawbacks</mark>:
    * ﻿﻿Does not scale well:
      * <mark style="color:yellow;">"Slow" memory shared by many "customers" (cores)</mark>
      * <mark style="color:yellow;">May become bottleneck (Amdahl's Law)</mark>
* ﻿﻿Two ways to use a multiprocessor:
  * ﻿﻿<mark style="color:yellow;">Job-level parallelism</mark>
    * ﻿﻿Processors work on unrelated problems
    * ﻿﻿No communication between programs
  * ﻿﻿<mark style="color:yellow;">Partition work of single task between several cores</mark>
    * ﻿﻿E.g., each performs part of large matrix multiplication

## Parallel Procession

* It's difficult!
* ﻿﻿It's inevitable
  * ﻿﻿<mark style="color:yellow;">Only path to increase performance</mark>
  * ﻿﻿<mark style="color:yellow;">Only path to lower energy consumption (improve battery life)</mark>
* ﻿﻿In mobile systems (e.g., smart phones, tablets)
  * ﻿﻿<mark style="color:yellow;">Muliple cores</mark>
  * ﻿﻿<mark style="color:yellow;">Dedicated processors</mark>, e.g.,
    * ﻿﻿<mark style="color:yellow;">Motion processor, image processor, neural processor</mark> in iPhone 8 + X
    * ﻿﻿<mark style="color:yellow;">GPU</mark> (graphics processing unit)
* ﻿﻿Warehouse-scale computers
  * ﻿﻿Multiple "nodes"
    * ﻿﻿"Boxes" with several CPUs, disks per box
  * ﻿﻿MIMD (multi-core) and SIMD (e.g. AVX) in each node

## Threads

* A <mark style="color:yellow;">Thread</mark> stands for "thread of execution", <mark style="color:yellow;">is a single stream of instructions</mark>
  * ﻿﻿<mark style="color:yellow;">A program / process can split, or fork itself into separate threads, which can (in theory) execute simultaneously.</mark>
  * ﻿﻿An easy way to describe/think about parallelism
* ﻿﻿With a single core, <mark style="color:yellow;">a single CPU can execute many threads by Time Sharing</mark>

<figure><img src=".gitbook/assets/image (258).png" alt=""><figcaption></figcaption></figure>

* Sequential flow of instructions that performs some task
  * ﻿﻿Up to now we just called this a "program"
* ﻿﻿Each <mark style="color:blue;">thread</mark> has:
  * ﻿﻿<mark style="color:yellow;">Dedicated PC (program counter)</mark>
  * ﻿﻿<mark style="color:yellow;">Separate registers</mark>
  * ﻿﻿<mark style="color:yellow;">Accesses the shared memory</mark>
* ﻿﻿Each <mark style="color:blue;">physical core</mark> provides one (or more)
  * ﻿﻿<mark style="color:yellow;">Hardware threads</mark> that actively execute instructions
  * ﻿﻿Each executes one " hardware thread"
* ﻿﻿<mark style="color:blue;">Operating system multiplexes</mark> multiple
  * ﻿﻿<mark style="color:yellow;">Software threads</mark> onto the available hardware threads
  * ﻿﻿<mark style="color:yellow;">All threads</mark> <mark style="color:purple;">**except those mapped to hardware threads**</mark> <mark style="color:yellow;">are waiting</mark>

## Operation System Threads

Give illusion of many "simultaneously" active threads

1. <mark style="color:yellow;">Multiplex software threads</mark> onto <mark style="color:yellow;">hardware threads</mark>:
   1. <mark style="color:yellow;">Switch out blocked threads</mark> (e.g., cache miss, user input, network access)
   2. <mark style="color:yellow;">Timer</mark> (e.g., switch active thread every 1 ms)
2. <mark style="color:yellow;">Remove a software thread</mark> from a hardware thread by
   1. ﻿<mark style="color:yellow;">Interrupting its execution</mark>
   2. ﻿﻿﻿<mark style="color:yellow;">Saving its registers and PC to memory</mark>
3. <mark style="color:yellow;">Start executing a different software</mark> thread by
   1. <mark style="color:yellow;">Loading its previously saved registers into a hardware thread's registers</mark>
   2. <mark style="color:yellow;">Jumping to its saved PC</mark>

<figure><img src=".gitbook/assets/image (8).png" alt="" width="563"><figcaption></figcaption></figure>

## MultiThreading

* Typical scenario:
  * ﻿﻿Active thread encounters cache miss
  * ﻿﻿Active thread waits \~ 1000 cycles for data from DRAM → switch out and run different thread until data available
* ﻿﻿Problem
  * ﻿﻿Must save current thread state and load new thread state
    * ﻿﻿PC, all registers (could be many, e.g. AVX) → must perform switch in « 1000 cycles
  * ﻿﻿<mark style="color:yellow;">Can hardware help?</mark>
    * ﻿﻿<mark style="color:yellow;">Moore's Law: transistors are plenty</mark>

## <mark style="color:yellow;">Hardware Asssited Software MultiThreading</mark>

* <mark style="color:yellow;">Two copies of PC and Registers</mark> inside processor hardware
* ﻿﻿Looks identical to two processors to software (<mark style="color:yellow;">hardware thread 0, hardware thread 1</mark>)
* ﻿﻿<mark style="color:yellow;">Hyper-Threading</mark>:
  * ﻿﻿Both threads can be active simultaneously

<figure><img src=".gitbook/assets/image (10).png" alt="" width="375"><figcaption></figcaption></figure>

## Hyper-Threading

* Simultaneous Multithreading (HT): <mark style="color:yellow;">Logical CPUs</mark> > <mark style="color:yellow;">Physical CPUs</mark>
  * Run multiple threads at the same time per core
  * Each thread has own architectural state (PC, Registers, etc.)
  * Share resources (cache, instruction unit, execution units)

## Multi-Threading

* <mark style="color:yellow;">Logical threads</mark>
  * ﻿﻿<mark style="color:yellow;">\~ 1% more hardware</mark>
  * ﻿﻿<mark style="color:red;">\~ 10% (?) better</mark> <mark style="color:yellow;">performance</mark>
    * ﻿﻿Separate registers
    * ﻿﻿Share datapath, ALU(s), caches
* ﻿﻿<mark style="color:yellow;">Multicore</mark>
  * ﻿﻿<mark style="color:yellow;">=> Duplicate Processors</mark>
  * ﻿﻿<mark style="color:yellow;">\~ 50% more hardware</mark>
  * ﻿﻿<mark style="color:yellow;">\~ 2X better performance?</mark>
* ﻿﻿Modern machines do both
  * ﻿﻿<mark style="color:yellow;">**Multiple cores with multiple threads per core**</mark>

## Example: 6 cores and 24 logical thread

<figure><img src=".gitbook/assets/image (13).png" alt="" width="563"><figcaption></figcaption></figure>

## Review

* Thread Level Parallelism
  * ﻿﻿<mark style="color:yellow;">Thread</mark>: sequence of instructions, with own program counter and processor state (e.g., register file)
  * ﻿﻿<mark style="color:yellow;">Multicore</mark>:
    * ﻿﻿<mark style="color:yellow;">Physical CPU</mark>: One thread (at a time) per CPU, in software OS switches threads typically in response , to 1/0 events like disk read/write
    * ﻿﻿<mark style="color:yellow;">Logical CPU</mark>: Fine-grain thread switching, in hardware, when thread blocks due to cache miss/memory access
    * ﻿﻿<mark style="color:yellow;">Hyper-Threading</mark> aka <mark style="color:yellow;">Simultaneous Multithreading (SMT)</mark>: Exploit superscalar architecture to launch instructions from different threads at the same time

## Conclusion

* Sequential software execution speed is limited
  * Clock rates flat or declining
* ﻿﻿Parallelism the only path to higher performance
  * SIMD: instruction level parallelism
    * ﻿﻿Implemented in all high perf. CPUs today (x86, ARM, ...)
    * ﻿﻿Partially supported by compilers
    * ﻿﻿2X width every 3-4 years
  * ﻿﻿MIMD: thread level parallelism
    * ﻿﻿Multicore processors
    * ﻿﻿Supported by Operating Systems (OS)
    * ﻿﻿Requires programmer intervention to exploit at single program level (we see later)
    * ﻿﻿Add 2 cores every 2 years (2, 4, 6, 8, 10, ...)
      * Intel Xeon W-3275: 28 Cores, 56 Threads
  * ﻿﻿SIMD & MIMD for maximum performance
* ﻿﻿Key challenge: craft parallel programs with high performance on multiprocessors as # of processors increase - i.e., that scale
  * ﻿﻿Scheduling, load balancing, time for synchronization, overhead communication
