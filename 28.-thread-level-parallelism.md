# 28. Thread Level Parallelism

## Improve Performance

1\. Increase clock rate fs

* ﻿﻿Reached practical maximum for today's technology
* ﻿﻿< 5GHz for general purpose computers

2\. Lower CPI (cycles per instruction)

• SIMD, "instruction level parallelism"

Today's lecture

3\. Perform multiple tasks simultaneously

• Multiple CPUs, each executing different program

^

* ﻿﻿Tasks may be related
* ﻿﻿E.g. each CPU performs part of a big matrix multiplication or unrelated
* ﻿﻿E.g. distribute different web hip requests over different computers
* ﻿﻿E.g. run pptx (view lecture slides) and browser (youtube)\
  simultaneously

4\. Do all of the above:

High fs, SIMD, multiple parallel tasks

<figure><img src=".gitbook/assets/image.png" alt=""><figcaption></figcaption></figure>

## Example: CPU with 2 Cores



<figure><img src=".gitbook/assets/image (1).png" alt=""><figcaption></figcaption></figure>

## Multiprocessor Execution Model

* Each processor (core) executes its own instructions\
  Separate resources (not shared)\
  Datapath (PC, registers, ALU)
* ﻿﻿Highest level caches (e.g., 1st and 2nd)
* ﻿﻿Shared resources
* ﻿﻿Memory (DRAM)
* ﻿﻿Often 3rd level cache
* ﻿﻿Often on same silicon chip
* ﻿﻿But not a requirement
* ﻿﻿Nomenclature
* ﻿﻿"Multiprocessor Microprocessor"
* ﻿﻿Multicore processor
* ﻿﻿E.g., four core CPU (central processing unit)
* ﻿﻿Executes four different instruction streams simultaneously

<figure><img src=".gitbook/assets/image (2).png" alt=""><figcaption></figcaption></figure>

* Shared memory
* ﻿﻿Each "core" has access to the entire memory in the processor
* ﻿﻿Special hardware keeps caches consistent (next lecture!)
* ﻿﻿Advantages:
* ﻿﻿Simplifies communication in program via shared variables
* ﻿﻿Drawbacks:
* ﻿﻿Does not scale well:

•

"Slow" memory shared by many "customers" (cores)

May become bottleneck (Amdahl's Law)

* ﻿﻿Two ways to use a multiprocessor:
* ﻿﻿Job-level parallelism
* ﻿﻿Processors work on unrelated problems
* ﻿﻿No communication between programs
* ﻿﻿Partition work of single task between several cores
* ﻿﻿E.g., each performs part of large matrix multiplication

<figure><img src=".gitbook/assets/image (4).png" alt=""><figcaption></figcaption></figure>

## Parallel Procession

* It's difficult!
* ﻿﻿It's inevitable
* ﻿﻿Only path to increase performance
* ﻿﻿Only path to lower energy consumption (improve battery life)
* ﻿﻿In mobile systems (e.g., smart phones, tablets)
* ﻿﻿Muliple cores
* ﻿﻿Dedicated processors, e.g.,
* ﻿﻿Motion processor, image processor, neural processor in iPhone 8
* ﻿﻿GPU (graphics processing unit)
* ﻿﻿Warehouse-scale computers (later in 61C!)
* ﻿﻿Multiple "nodes"
* ﻿﻿"Boxes" with several CPUs, disks per box
* ﻿﻿MIMD (multi-core) and SIMD (e.g. AVX) in each node

<figure><img src=".gitbook/assets/image (3).png" alt=""><figcaption></figcaption></figure>

## FLOPS

## Threads

A Thread stands for "thread of execution", is a single stream of instructions

Process

* ﻿﻿A program / process can split, or fork itself into separate threads, which can (in theory) execute simultaneously.
* ﻿﻿An easy way to describe/think about parallelism
* ﻿﻿With a single core, a single CPU can execute many threads by Time Sharing

<figure><img src=".gitbook/assets/image (5).png" alt=""><figcaption></figcaption></figure>

* Sequential flow of instructions that performs some task
* ﻿﻿Up to now we just called this a "program"
* ﻿﻿Each thread has:
* ﻿﻿Dedicated PC (program counter)
* ﻿﻿Separate registers
* ﻿﻿Accesses the shared memory
* ﻿﻿Each physical core provides one (or more)
* ﻿﻿Hardware threads that actively execute instructions
* ﻿﻿Each executes one " hardware thread"
* ﻿﻿Operating system multiplexes multiple
* ﻿﻿Software threads onto the available hardware threads
* ﻿﻿All threads except those mapped to hardware threads are waiting

<figure><img src=".gitbook/assets/image (6).png" alt=""><figcaption></figcaption></figure>

## Operation System Threads

Give illusion of many "simultaneously" active threads

1\. Multiplex software threads onto hardware threads:

a)

Switch out blocked threads le.g., cache miss, user input, network access)

b)

Timer (e.g., switch active thread every 1 ms)

2\. Remove a software thread from a hardware thread by

1. ﻿﻿﻿Interrupting its execution
2. ﻿﻿﻿Saving its registers and PC to memory

3\. Start executing a different software thread by a)

Loading its previously saved registers into a hardware thread's registers

b)

Jumping to its saved PC

<figure><img src=".gitbook/assets/image (7).png" alt=""><figcaption></figcaption></figure>

<figure><img src=".gitbook/assets/image (8).png" alt=""><figcaption></figcaption></figure>

## MultiThreading

* Typical scenario:
* ﻿﻿Active thread encounters cache miss
* ﻿﻿Active thread waits \~ 1000 cycles for data from DRAM → switch out and run different thread until data available
* ﻿﻿Problem

丷

* ﻿﻿Must save current thread state and load new thread state
* ﻿﻿PC, all registers (could be many, e.g. AVX)

→ must perform switch in « 1000 cycles

* ﻿﻿Can hardware help?
* ﻿﻿Moore's Law: transistors are plenty

<figure><img src=".gitbook/assets/image (9).png" alt=""><figcaption></figcaption></figure>

Hardware Asssited Software MultiThreading

* Two copies of PC and Registers inside processor hardware
* ﻿﻿Looks identical to two processors to software (hardware thread 0, hardware thread 1)
* ﻿﻿Hyper-Threading:
* ﻿﻿Both threads can be active simultaneously

<figure><img src=".gitbook/assets/image (10).png" alt=""><figcaption></figcaption></figure>

## Hyper-Threading

Simultaneous Multithreading (HT): Logical CPUs > Physical CPUs

Run multiple threads at the same time per core

Each thread has own architectural state (PC, Registers, etc.)

Share resources (cache, instruction unit, execution units)

See http://dada.cs.washington.edu/smt/

<figure><img src=".gitbook/assets/image (11).png" alt=""><figcaption></figcaption></figure>

## Multi-Threading

* Logical threads
* ﻿﻿\~ 1% more hardware
* ﻿﻿\~ 10% (?) better performance
* ﻿﻿Separate registers
* ﻿﻿Share datapath, ALU(s), caches
* ﻿﻿Multicore
* ﻿﻿=> Duplicate Processors
* ﻿﻿\~ 50% more hardware
* ﻿﻿\~ 2X better performance?
* ﻿﻿Modern machines do both
* ﻿﻿Multiple cores with multiple threads per core

<figure><img src=".gitbook/assets/image (12).png" alt=""><figcaption></figcaption></figure>

## Example: 6 cores and 24 logical thread

<figure><img src=".gitbook/assets/image (13).png" alt=""><figcaption></figcaption></figure>

## Review

* Thread Level Parallelism
* ﻿﻿Thread: sequence of instructions, with own program counter and processor state (e.g., register file)
* ﻿﻿Multicore:
* ﻿﻿Physical CPU: One thread (at a time) per CPU, in software OS switches threads typically in response , to 1/0 events like disk read/write
* ﻿﻿Logical CPU: Fine-grain thread switching, in hardware, when thread blocks due to cache miss/memory access
* ﻿﻿Hyper-Threading aka Simultaneous Multithreading

(SMT): Exploit superscalar architecture to launch instructions from different threads at the same time!

<figure><img src=".gitbook/assets/image (14).png" alt=""><figcaption></figcaption></figure>

## Conclusion

* Sequential software execution speed is limited\
  Clock rates flat or declining
* ﻿﻿Parallelism the only path to higher performance\
  SIMD: instruction level parallelism
* ﻿﻿Implemented in all high perf. CPUs today (x86, ARM, ...)
* ﻿﻿Partially supported by compilers
* ﻿﻿2X width every 3-4 years
* ﻿﻿MIMD: thread level parallelism
* ﻿﻿Multicore processors
* ﻿﻿Supported by Operating Systems (OS)
* ﻿﻿Requires programmer intervention to exploit at single program level (we see later)
* ﻿﻿Add 2 cores every 2 years (2, 4, 6, 8, 10, ...)

•

Intel Xeon W-3275: 28 Cores, 56 Threads

* ﻿﻿SIMD & MIMD for maximum performance
* ﻿﻿Key challenge: craft parallel programs with high performance on multiprocessors as # of processors increase - i.e., that scale
* ﻿﻿Scheduling, load balancing, time for synchronization, overhead communication

<figure><img src=".gitbook/assets/image (15).png" alt=""><figcaption></figcaption></figure>







