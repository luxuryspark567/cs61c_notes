# 28. Thread Level Parallelism

## Improve Performance

1. <mark style="color:yellow;">Increase clock rate</mark>
   1. ﻿﻿Reached practical maximum for today's technology
   2. <mark style="color:yellow;">< 5GHz for general purpose computers</mark>
2. <mark style="color:yellow;">Lower CPI (cycles per instruction)</mark>
   1. SIMD, "instruction level parallelism"
3. <mark style="color:yellow;">Perform multiple tasks simultaneously</mark>
   1. Multiple CPUs, each executing different program
   2. ﻿﻿Tasks may be related
      1. E.g. each CPU performs part of a big matrix multiplication&#x20;
   3. or unrelated
      1. E.g. distribute different web hip requests over different computers
      2. E.g. run pptx (view lecture slides) and browser (youtube)\
         simultaneously

## Example: CPU with 2 Cores

<figure><img src=".gitbook/assets/image (1).png" alt="" width="375"><figcaption></figcaption></figure>

## Multiprocessor Execution Model

Each processor (core) executes its own instructions

* Separate resources (not shared)
  * Datapath (PC, registers, ALU)
  * ﻿﻿Highest level caches (e.g., 1st and 2nd)
* ﻿﻿Shared resources
  * ﻿﻿Memory (DRAM)
  * ﻿﻿Often 3rd level cache
    * ﻿﻿Often on same silicon chip
    * ﻿﻿But not a requirement
* ﻿﻿Nomenclature
  * ﻿﻿"Multiprocessor Microprocessor"
  * ﻿﻿Multicore processor
    * ﻿﻿E.g., four core CPU (central processing unit)
    * ﻿﻿Executes four different instruction streams simultaneously
* Shared memory
  * ﻿﻿Each "core" has access to the entire memory in the processor
  * ﻿﻿Special hardware keeps caches consistent
  * ﻿﻿Advantages:
    * ﻿﻿Simplifies communication in program via shared variables
  * ﻿﻿Drawbacks:
    * ﻿﻿Does not scale well:
      * "Slow" memory shared by many "customers" (cores)
      * May become bottleneck (Amdahl's Law)
* ﻿﻿Two ways to use a multiprocessor:
  * ﻿﻿Job-level parallelism
    * ﻿﻿Processors work on unrelated problems
    * ﻿﻿No communication between programs
  * ﻿﻿Partition work of single task between several cores
    * ﻿﻿E.g., each performs part of large matrix multiplication

## Parallel Procession

* It's difficult!
* ﻿﻿It's inevitable
  * ﻿﻿Only path to increase performance
  * ﻿﻿Only path to lower energy consumption (improve battery life)
* ﻿﻿In mobile systems (e.g., smart phones, tablets)
  * ﻿﻿Muliple cores
  * ﻿﻿Dedicated processors, e.g.,
    * ﻿﻿Motion processor, image processor, neural processor in iPhone 8 + X
    * ﻿﻿GPU (graphics processing unit)
* ﻿﻿Warehouse-scale computers
  * ﻿﻿Multiple "nodes"
    * ﻿﻿"Boxes" with several CPUs, disks per box
  * ﻿﻿MIMD (multi-core) and SIMD (e.g. AVX) in each node

## <mark style="color:yellow;">FLOPS</mark>

## Threads

* A Thread stands for "thread of execution", is a single stream of instructions
  * ﻿﻿A program / process can split, or fork itself into separate threads, which can (in theory) execute simultaneously.
  * ﻿﻿An easy way to describe/think about parallelism
* ﻿﻿With a single core, a single CPU can execute many threads by Time Sharing

<figure><img src=".gitbook/assets/image (258).png" alt=""><figcaption></figcaption></figure>

* Sequential flow of instructions that performs some task
  * ﻿﻿Up to now we just called this a "program"
* ﻿﻿Each thread has:
  * ﻿﻿Dedicated PC (program counter)
  * ﻿﻿Separate registers
  * ﻿﻿Accesses the shared memory
* ﻿﻿Each physical core provides one (or more)
  * ﻿﻿<mark style="color:yellow;">Hardware threads</mark> that actively execute instructions
  * ﻿﻿Each executes one " hardware thread"
* ﻿﻿Operating system multiplexes multiple
  * ﻿﻿<mark style="color:yellow;">Software threads</mark> onto the available hardware threads
  * ﻿﻿All threads except those mapped to hardware threads are waiting

## Operation System Threads

Give illusion of many "simultaneously" active threads

1. Multiplex software threads onto hardware threads:
   1. Switch out blocked threads (e.g., cache miss, user input, network access)
   2. Timer (e.g., switch active thread every 1 ms)
2. Remove a software thread from a hardware thread by
   1. ﻿Interrupting its execution
   2. ﻿﻿﻿Saving its registers and PC to memory
3. Start executing a different software thread by
   1. Loading its previously saved registers into a hardware thread's registers
   2. Jumping to its saved PC

<figure><img src=".gitbook/assets/image (8).png" alt=""><figcaption></figcaption></figure>

## MultiThreading

* Typical scenario:
  * ﻿﻿Active thread encounters cache miss
  * ﻿﻿Active thread waits \~ 1000 cycles for data from DRAM → switch out and run different thread until data available
* ﻿﻿Problem
  * ﻿﻿Must save current thread state and load new thread state
    * ﻿﻿PC, all registers (could be many, e.g. AVX) → must perform switch in « 1000 cycles
  * ﻿﻿Can hardware help?
    * ﻿﻿Moore's Law: transistors are plenty

## Hardware Asssited Software MultiThreading

* Two copies of PC and Registers inside processor hardware
* ﻿﻿Looks identical to two processors to software (hardware thread 0, hardware thread 1)
* ﻿﻿Hyper-Threading:
  * ﻿﻿Both threads can be active simultaneously

<figure><img src=".gitbook/assets/image (10).png" alt="" width="375"><figcaption></figcaption></figure>

## Hyper-Threading

* Simultaneous Multithreading (HT): Logical CPUs > Physical CPUs
  * Run multiple threads at the same time per core
  * Each thread has own architectural state (PC, Registers, etc.)
  * Share resources (cache, instruction unit, execution units)
  * See http://dada.cs.washington.edu/smt/

## Multi-Threading

* Logical threads
  * ﻿﻿\~ 1% more hardware
  * ﻿﻿\~ 10% (?) better performance
    * ﻿﻿Separate registers
    * ﻿﻿Share datapath, ALU(s), caches
* ﻿﻿Multicore
  * ﻿﻿=> Duplicate Processors
  * ﻿﻿\~ 50% more hardware
  * ﻿﻿\~ 2X better performance?
* ﻿﻿Modern machines do both
  * ﻿﻿Multiple cores with multiple threads per core

## Example: 6 cores and 24 logical thread

<figure><img src=".gitbook/assets/image (13).png" alt="" width="563"><figcaption></figcaption></figure>

## Review

* Thread Level Parallelism
  * ﻿﻿<mark style="color:yellow;">Thread</mark>: sequence of instructions, with own program counter and processor state (e.g., register file)
  * ﻿﻿<mark style="color:yellow;">Multicore</mark>:
    * ﻿﻿<mark style="color:yellow;">Physical CPU</mark>: One thread (at a time) per CPU, in software OS switches threads typically in response , to 1/0 events like disk read/write
    * ﻿﻿<mark style="color:yellow;">Logical CPU</mark>: Fine-grain thread switching, in hardware, when thread blocks due to cache miss/memory access
    * ﻿﻿<mark style="color:yellow;">Hyper-Threading</mark> aka <mark style="color:yellow;">Simultaneous Multithreading (SMT)</mark>: Exploit superscalar architecture to launch instructions from different threads at the same time

## Conclusion

* Sequential software execution speed is limited
  * Clock rates flat or declining
* ﻿﻿Parallelism the only path to higher performance
  * SIMD: instruction level parallelism
    * ﻿﻿Implemented in all high perf. CPUs today (x86, ARM, ...)
    * ﻿﻿Partially supported by compilers
    * ﻿﻿2X width every 3-4 years
  * ﻿﻿MIMD: thread level parallelism
    * ﻿﻿Multicore processors
    * ﻿﻿Supported by Operating Systems (OS)
    * ﻿﻿Requires programmer intervention to exploit at single program level (we see later)
    * ﻿﻿Add 2 cores every 2 years (2, 4, 6, 8, 10, ...)
      * Intel Xeon W-3275: 28 Cores, 56 Threads
  * ﻿﻿SIMD & MIMD for maximum performance
* ﻿﻿Key challenge: craft parallel programs with high performance on multiprocessors as # of processors increase - i.e., that scale
  * ﻿﻿Scheduling, load balancing, time for synchronization, overhead communication
