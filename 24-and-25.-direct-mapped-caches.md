# 24&25. Direct Mapped Caches

In a direct-mapped cache, <mark style="color:yellow;">each memory address is associated with one possible block within the cache</mark>&#x20;

Therefore, we only need to look in a single location in the cache for the data if it exists in the cache&#x20;

<mark style="color:yellow;">Block is the unit of transfer between cache and memory</mark>

## Terminology

All fields are read as unsigned integers.&#x20;

* Index: specifies the cache index (which “row”/block of the cache we should look in)&#x20;
* Offset: once we’ve found correct block, specifies which byte within the block we want&#x20;
* Tag: the remaining bits after offset and index are determined; these are used to distinguish between all the memory addresses that map to the same location

## TIO Cache Mnemonic

<figure><img src=".gitbook/assets/image (241).png" alt=""><figcaption></figcaption></figure>

## Example

Suppose we have a 8B of data in a direct-mapped cache with 2-byte blocks, determine the size of the tag, index and offset fields if using a 32-bit arch (RV32)

* Offset: need to specify correct byte within a block, block contains 2 bytes, need 1 bit to specify correct byte;
* Index: need to specify correct block in cache, cache contains 8B/2B = 4 blocks/cache, need 2 bits to specify this many blocks
* Tag: use remaining bits as tag tag: 32 - 1 - 2 bits = 29 bits, so tag is leftmost 29 bits of memory address

## Memory Access without Cache

Load word instruction: lw t0, 0(t1), t1 contains 1022\_ten , Memory\[1022] = 99

* Processor issues address 1022\_ten to Memory&#x20;
* Memory reads word at address 1022\_ten (99)&#x20;
* Memory sends 99 to Processor&#x20;
* Processor loads 99 into register t0

Memory Access with Cache

Load word instruction: lw t0, 0(t1), t1 contains 1022\_ten , Memory\[1022] = 99

1. Processor issues address 1022\_ten to Cache
2. Cache checks to see if has copy of data at address 1022\_ten
   1. If finds a match (Hit): cache reads 99, sends to processor
   2. No match (Miss): cache sends address 1022 to Memory
      1. Memory reads 99 at address 1022\_ten
      2. Memory sends 99 to Cache
      3. Cache replaces word with new 99
      4. Cache sends 99 to processor
3. Processor loads 99 into register t0

## Cache Structure

<figure><img src=".gitbook/assets/image (242).png" alt=""><figcaption></figcaption></figure>

## Terminology

* cache hit: cache block is valid and contains proper address, so read desired word&#x20;
* cache miss: nothing in cache in appropriate block, so fetch from memory&#x20;
* <mark style="color:yellow;">cache miss, block replacement</mark>: wrong data is in cache at appropriate block, so discard it and fetch desired data from memory (cache always copy)
* Cold: Cache empty&#x20;
* Warming: Cache filling with values you’ll hopefully be accessing again soon&#x20;
* Warm: Cache is doing its job, fair percent of hits&#x20;
* Hot: Cache is doing very well, high percent of hits
* Hit rate: fraction of access that hit in the cache&#x20;
* Miss rate: 1 – Hit rate&#x20;
* Miss penalty: time to replace a block from lower level in memory hierarchy to cache&#x20;
* Hit time: time to access cache memory (including tag comparison)

## One More Detail: Valid Bit

Add a “valid bit” to the cache tag entry

* 0: cache miss, even if by chance, address = tag
* 1: cache hit, if processor address = tag

## Example: 16 KB Direct-Mapped Cache, 16B blocks

Valid bit: determines whether anything is stored in that row (when computer initially powered up, all entries invalid)

<figure><img src=".gitbook/assets/image (243).png" alt=""><figcaption></figcaption></figure>

## Example

Ex.: 16KB of data, direct-mapped, 4 word blocks

* Can you work out height, width, area?&#x20;
  * Offset: 4 words = 16B = 2^4 \~ 4bits wide
  * Index: 16KB / 16B = 1024 = 2^10 \~ 10bits wide
  * Tag: 32 - 4 - 10 = 18bits wide
* Read 4 addresses
  * 0x00000014
  * 0x0000001C
  * 0x00000034
  * 0x00008014

<figure><img src=".gitbook/assets/image (244).png" alt="" width="375"><figcaption></figcaption></figure>



* Read 0x00000014 (000000000000000000 0000000001 0100)
* read block 1 (0000000001): No valid data
* Load that data into cache, setting tag, valid
* Read from cache at offset, return word b

<figure><img src=".gitbook/assets/image (245).png" alt=""><figcaption></figcaption></figure>

* Read 0x0000001C (000000000000000000 0000000001 1100)
* Index is Valid, Tag Matches
* Read from cache at offset, return d



* Read 0x00000034 = 000000000000000000 0000000011 0100
* read block 3: No valid data
* Load that cache block, return word f

<figure><img src=".gitbook/assets/image (246).png" alt=""><figcaption></figcaption></figure>

* Read 0x00008014 (000000000000000010 0000000001 0100)
* read Cache Block 1, Data is Valid
* Cache Block 1 Tag does not match (0 ≠ 2)
* Miss, so replace block 1 with new data & tag
* return word j

<figure><img src=".gitbook/assets/image (247).png" alt=""><figcaption></figcaption></figure>

## Multiword-Block Direct-Mapped Cache

Four words/block, cache size = 4K words

<figure><img src=".gitbook/assets/image (248).png" alt=""><figcaption></figcaption></figure>

## What to do on a write hit?

* Write-through&#x20;
  * Update both cache and memory&#x20;
* Write-back&#x20;
  * update word in cache block&#x20;
  * allow memory word to be “stale”
  * add ‘dirty’ bit to block&#x20;
    * memory & Cache inconsistent&#x20;
    * needs to be updated when block is replaced&#x20;
  * …OS flushes cache before I/O…

## Block Size Tradeoff

* Benefits
  * Spatial Locality: if we access a given word, we’re likely to access other nearby words soon&#x20;
  * Very applicable with Stored-Program Concept&#x20;
  * Works well for sequential array accesses&#x20;
* Drawbacks
  * Larger block size means larger miss penalty&#x20;
    * on a miss, takes longer time to load a new block from next level&#x20;
  * If block size is too big relative to cache size, then there are too few blocks&#x20;
    * Result: miss rate goes up

## Extreme Example: One Big Block

<figure><img src=".gitbook/assets/image (249).png" alt=""><figcaption></figcaption></figure>

* Cache Size = 4 bytes Block Size = 4 bytes&#x20;
  * Only ONE entry (row) in the cache!&#x20;
* If item accessed, likely accessed again soon&#x20;
  * But unlikely will be accessed again immediately!&#x20;
* The next access will likely to be a miss again&#x20;
  * Continually loading data into the cache but discard data (force out) before use it again&#x20;
  * Nightmare for cache designer: Ping Pong Effect

<figure><img src=".gitbook/assets/image (250).png" alt=""><figcaption></figcaption></figure>

## Types of Cache Misses

* 1st C: Compulsory Misses&#x20;
  * occur when a program is first started&#x20;
  * cache does not contain any of that program’s data yet, so misses are bound to occur&#x20;
  * can’t be avoided easily, so won’t focus on these in this course&#x20;
  * Every block of memory will have one compulsory miss (NOT only every block of the cache)
* 2nd C: Conflict Misses&#x20;
  * miss that occurs because two distinct memory addresses map to the same cache location&#x20;
  * two blocks (which happen to map to the same location) can keep overwriting each other&#x20;
  * big problem in direct-mapped caches&#x20;
  * how do we lessen the effect of these?&#x20;
* Dealing with Conflict Misses&#x20;
  * Solution 1: Make the cache size bigger Fails at some point&#x20;
  * Solution 2: Multiple distinct blocks can fit in the same cache Index?

## Fully Associative Caches

* Memory address fields:&#x20;
  * Tag: same as before&#x20;
  * Offset: same as before&#x20;
  * Index: non-existant&#x20;
* What does this mean?&#x20;
  * no “rows”: any block can go anywhere in the cache&#x20;
  * must compare with all tags in entire cache to see if data is there
* compare tags in parallel

<figure><img src=".gitbook/assets/image (251).png" alt=""><figcaption></figcaption></figure>

* Benefit
  * No Conflict Misses (since data can go anywhere)&#x20;
* Drawbacks
  * Need hardware comparator for every single entry: if we have a 64KB of data in cache with 4B entries, we need 16K comparators: infeasible



* 3rd C: Capacity Misses&#x20;
  * miss that occurs because the cache has a limited size&#x20;
  * miss that would not occur if we increase the size of the cache&#x20;
  * sketchy definition, so just get the general idea&#x20;
* This is the primary type of miss for Fully Associative caches.

## How to categorize misses

Run an address trace against a set of caches:

* First, consider an infinite-size, fully-associative cache. For every miss that occurs now, consider it a compulsory miss.&#x20;
* Next, consider a finite-sized cache (of the size you want to examine) with full-associativity. Every miss that is not in #1 is a capacity miss.&#x20;
* Finally, consider a finite-size cache with finite-associativity. All of the remaining misses that are not #1 or #2 are conflict misses.
