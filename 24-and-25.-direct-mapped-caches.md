# 24 & 25. Direct Mapped Caches

In a direct-mapped cache, <mark style="color:yellow;">each memory address is associated with one possible block within the cache.</mark>

Therefore, we only need to look in a single location in the cache for the data if it exists in the cache&#x20;

<mark style="color:yellow;">Block is the unit of transfer between cache and memory.</mark>

## Terminology

All fields are read as unsigned integers.&#x20;

* <mark style="color:red;">Index</mark>: specifies the cache index (<mark style="color:yellow;">which “row”/block</mark>)&#x20;
* <mark style="color:red;">Offset</mark>: once we’ve found correct block, specifies which byte within the block we want(<mark style="color:yellow;">which "col"/ byte</mark>)
* <mark style="color:red;">Tag</mark>: the remaining bits after offset and index are determined; these are <mark style="color:yellow;">used to distinguish between all the memory addresses that</mark> <mark style="color:red;">map to the same location.</mark>
  * 下面的AREA就是值得Cache
  * 映射关系可以认为是将Memory均分成Cache大小的块
  * Tag是均分之后的每小块Area的编号，映射到同一位置的Memory Address通过Tag进行区分。
* <mark style="color:red;">block</mark>: 一个block对应下面的one row，总共有HEIGHT个row。cache是以block为单位进行更新的。

## <mark style="color:red;">T</mark><mark style="color:green;">I</mark><mark style="color:blue;">O</mark> Cache Mnemonic

<figure><img src=".gitbook/assets/image (241).png" alt="" width="563"><figcaption></figcaption></figure>

## Memory Access without Cache

Load word instruction: lw t0, 0(t1), t1 contains $$1022_{10}$$ , Memory\[1022] = 99

* Processor issues address $$1022_{10}$$ to Memory&#x20;
* Memory reads word at address $$1022_{10}$$(99)&#x20;
* Memory sends 99 to Processor&#x20;
* Processor loads 99 into register t0

## Memory Access with Cache

Load word instruction: lw t0, 0(t1), t1 contains $$1022_{10}$$ , Memory\[1022] = 99

1. Processor issues address $$1022_{10}$$ to Cache
2. Cache checks to see if has copy of data at address $$1022_{10}$$
   1. If finds a match (<mark style="color:red;">Hit</mark>): cache reads 99, sends to processor
   2. <mark style="color:yellow;">No match (</mark><mark style="color:red;">Miss</mark><mark style="color:yellow;">)</mark>: cache sends address 1022 to Memory
      1. <mark style="color:blue;">Memory reads</mark> 99 at address $$1022_{10}$$
      2. <mark style="color:blue;">Memory sends</mark> 99 to Cache
      3. <mark style="color:yellow;">Cache replaces</mark> word with new 99
      4. <mark style="color:yellow;">Cache sends</mark> 99 to processor
3. Processor loads 99 into register t0

## Cache Structure

<mark style="color:red;">**MEMORY被分成CACHE大小的AREA**</mark>

<figure><img src=".gitbook/assets/image (242).png" alt=""><figcaption></figcaption></figure>

## Terminology

* <mark style="color:yellow;">cache hit</mark>: cache block is valid and contains proper address, so read desired word&#x20;
* <mark style="color:yellow;">cache miss</mark>: nothing in cache in appropriate block, so fetch from memory&#x20;
* <mark style="color:yellow;">cache miss, block replacement</mark>: wrong data is in cache at appropriate block, so discard it and fetch desired data from memory (cache always copy)
* <mark style="color:yellow;">Cold</mark>: Cache empty&#x20;
* <mark style="color:yellow;">Warming</mark>: Cache filling with values you’ll hopefully be accessing again soon&#x20;
* <mark style="color:yellow;">Warm</mark>: Cache is doing its job, fair percent of hits&#x20;
* <mark style="color:yellow;">Hot</mark>: Cache is doing very well, high percent of hits
* <mark style="color:yellow;">Hit rate</mark>: fraction of access that hit in the cache&#x20;
* <mark style="color:yellow;">Miss rate</mark>: (1 – Hit rate)
* <mark style="color:yellow;">Miss penalty</mark>: <mark style="color:red;">time to replace a block</mark> <mark style="color:yellow;">from lower level in memory hierarchy to cache</mark>&#x20;
* <mark style="color:yellow;">Hit time</mark>: <mark style="color:red;">time to access cache memory</mark> <mark style="color:yellow;">(including tag comparison)</mark>

## One More Detail: <mark style="color:red;">Valid Bit</mark>

Add a “valid bit” to the cache tag entry

* <mark style="color:red;">0: entry invalid</mark>
* <mark style="color:red;">1: valid</mark>

## Example: 16 KB Direct-Mapped Cache, 16B blocks

Valid bit: Determines whether anything is stored in that row (when computer initially powered up, all entries invalid)

<figure><img src=".gitbook/assets/image (243).png" alt=""><figcaption></figcaption></figure>

## Example

Ex.: **16KB** of data, direct-mapped, **4 word blocks**

* Can you work out height, width, area?&#x20;
  * <mark style="color:blue;">Offset</mark>: 4 words = 16B = 2^4 ⇒ 4bits wide
  * <mark style="color:green;">Index</mark>: 16KB / 16B = 1024 = 2^10 ⇒  10bits wide
  * <mark style="color:red;">Tag</mark>: 32 - 4 - 10 = 18bits wide
* Read 4 addresses
  * 0x00000014
  * 0x0000001C
  * 0x00000034
  * 0x00008014

<figure><img src=".gitbook/assets/image (244).png" alt="" width="375"><figcaption></figcaption></figure>



* Read 0x00000014 (000000000000000000 0000000001 0100)
* read block 1 (0000000001): No valid data
* Load that data into cache, setting tag, valid
* Read from cache at offset, return word b

<figure><img src=".gitbook/assets/image (245).png" alt=""><figcaption></figcaption></figure>

* Read 0x0000001C (000000000000000000 0000000001 1100)
* Index is Valid, Tag Matches
* Read from cache at offset, return d



* Read 0x00000034 = 000000000000000000 0000000011 0100
* read block 3: No valid data
* Load that cache block, return word f

<figure><img src=".gitbook/assets/image (246).png" alt=""><figcaption></figcaption></figure>

* Read 0x00008014 (000000000000000010 0000000001 0100)
* read Cache Block 1, Data is Valid
* Cache Block 1 Tag does not match (0 ≠ 2)
* Miss, so replace block 1 with new data & tag
* return word j

<figure><img src=".gitbook/assets/image (247).png" alt=""><figcaption></figcaption></figure>

## Multiword-Block Direct-Mapped Cache

Four words/block, cache size = 4K words

<figure><img src=".gitbook/assets/image (248).png" alt=""><figcaption></figcaption></figure>

## What to do on a write hit?

* <mark style="color:red;">**Write-through**</mark>&#x20;
  * <mark style="color:red;">Update both cache and memory</mark>&#x20;
* <mark style="color:red;">**Write-back**</mark>&#x20;
  * update word in cache block&#x20;
  * allow memory word to be “**stale**”
  * <mark style="color:yellow;">add</mark> <mark style="color:yellow;"></mark><mark style="color:yellow;">**‘**</mark><mark style="color:red;">**dirty**</mark><mark style="color:yellow;">**’ bit**</mark> <mark style="color:yellow;"></mark><mark style="color:yellow;">to block</mark>&#x20;
    * Memory & Cache inconsistent&#x20;
    * <mark style="color:red;">**needs to be updated when block is replaced**</mark>&#x20;
  * …<mark style="color:yellow;">OS flushes cache before I/O</mark>…

## Block Size Tradeoff

* Benefits of Larger Block Size
  * <mark style="color:red;">Spatial Locality</mark>: if we access a given word, we’re likely to access other nearby words soon&#x20;
  * Very applicable with Stored-Program Concept&#x20;
  * Works well for <mark style="color:yellow;">sequential array accesses</mark>&#x20;
* Drawbacks of Larger Block Size
  * <mark style="color:yellow;">Larger block size means</mark> <mark style="color:red;">**larger miss penalty**</mark>&#x20;
    * On a miss, <mark style="color:yellow;">takes longer time to load a new block</mark> from next level&#x20;
  * If block size is <mark style="color:yellow;">too big</mark> relative to cache size, then there are too few blocks&#x20;
    * Result: <mark style="color:yellow;">miss rate goes up</mark>

## Extreme Example: One Big Block

<figure><img src=".gitbook/assets/image (249).png" alt="" width="563"><figcaption></figcaption></figure>

* Cache Size = 4 bytes, Block Size = 4 bytes&#x20;
  * Only ONE entry (row) in the cache!&#x20;
* If item accessed, likely accessed again soon&#x20;
  * But unlikely will be accessed again immediately!&#x20;
* The next access will likely to be a miss again&#x20;
  * Continually loading data into the cache but discard data (force out) before use it again&#x20;
  * Nightmare for cache designer: <mark style="color:yellow;">**Ping Pong Effect**</mark>

<figure><img src=".gitbook/assets/image (250).png" alt="" width="563"><figcaption></figcaption></figure>

## Types of Cache Misses

* _<mark style="color:red;">**Conflict miss**</mark>: If the address <mark style="color:red;">was in the cache</mark>, but isn't anymore (got evicted) and the <mark style="color:red;">cache is still not full</mark>._
* _<mark style="color:red;">**Capacity miss**</mark>: If the address <mark style="color:red;">was in the cache</mark>, but isn't anymore (got evicted) and the <mark style="color:red;">cache is full</mark>._
* _<mark style="color:red;">**Compulsory miss**</mark>: If the address <mark style="color:red;">was never in the cache</mark> (cold start)._



* 1st C: <mark style="color:yellow;">Compulsory Misses</mark>&#x20;
  * occur when a program is first started&#x20;
  * <mark style="color:yellow;">cache does not contain any of that program’s data yet, so misses are bound to occur</mark>&#x20;
  * Every block of memory will have one compulsory miss (NOT only every block of the cache)
* 2nd C: <mark style="color:yellow;">Conflict Misses</mark>&#x20;
  * miss that occurs because <mark style="color:red;">two distinct memory addresses map to the same cache location</mark><mark style="color:yellow;">, two blocks (which happen to map to the same location) can keep overwriting each other</mark>&#x20;
* Dealing with Conflict Misses&#x20;
  * Solution 1: <mark style="color:yellow;">Make the cache size bigger</mark>, Fails at some point&#x20;
  * Solution 2: Multiple distinct blocks can fit in the same cache Index, use <mark style="color:red;">Fully Associative Caches</mark>.

## Fully Associative Caches

* Fully Associative Cache
  * Memory被分成Block大小的单元
  * Cache被分成Block大小的单元
  * 必须要比较所有的条目，知道找到对应的条目；
* Direct Mapped CachesCache
  * Memory被分成Cache Area大小的单元，Area由Index\_Max个Block组成。
  * Cache被分成Block大小的单元
  * 根据地址信息可以精确的找到对应的block中的条目



* Memory address fields:&#x20;
  * Tag: same as before&#x20;
  * Offset: same as before&#x20;
  * <mark style="color:red;">**Index: non-existant**</mark>&#x20;
* What does this mean?&#x20;
  * <mark style="color:yellow;">no “rows”: any block can go anywhere in the cache</mark>&#x20;
  * <mark style="color:yellow;">must compare with all tags in entire cache to see if data is there</mark>
* compare tags in parallel

<figure><img src=".gitbook/assets/image (251).png" alt="" width="563"><figcaption></figcaption></figure>

* Benefit
  * No Conflict Misses (since data can go anywhere)&#x20;
* Drawbacks
  * <mark style="color:yellow;">Need hardware comparator for every single entry:</mark> if we have a 64KB of data in cache with 4B entries, <mark style="color:yellow;">we need 16K comparators: infeasible</mark>
* 3rd C: <mark style="color:yellow;">Capacity Misses</mark>&#x20;
  * <mark style="color:yellow;">miss that occurs because the cache has a limited size</mark>&#x20;
  * miss that would not occur if we increase the size of the cache&#x20;
  * sketchy definition, so just get the general idea&#x20;
* This is the primary type of miss for Fully Associative caches.

## How to categorize misses

Run an address trace against a set of caches:

* First, consider an infinite-size, fully-associative cache. For every miss that occurs now, consider it a <mark style="color:yellow;">compulsory miss.</mark>&#x20;
* Next, consider a finite-sized cache (of the size you want to examine) with full-associativity. Every miss that is not in #1 is a capacity miss.&#x20;
* Finally, consider a finite-size cache with finite-associativity. All of the remaining misses that are not #1 or #2 are <mark style="color:yellow;">conflict misses.</mark>
