# 29. OpenMP, Sharing lssues

## Why So Many Parallel Programming Languages?

* ﻿﻿Why "intrinsics"?
  * ﻿﻿TO Intel: fix your compiler, thanks...
* ﻿﻿It's happening ... but
  * ﻿﻿SIMD features are continually added to compilers (Intel, gcc)
  * Intense area of research
  * ﻿﻿Research progress:
    * ﻿﻿<mark style="color:yellow;">20+ years to translate C into good (fast) assembly</mark>
    * ﻿﻿How long to <mark style="color:yellow;">translate C into good (fast) parallel code</mark>?
      * <mark style="color:red;">General problem is very hard to solve</mark>
      * Present state: <mark style="color:yellow;">specialized solutions for specific cases</mark>

## Parallel Programming Languages

* ﻿﻿Number of choices is indication of
  * ﻿﻿No universal solution, needs are very <mark style="color:red;">problem specific</mark>
    * ﻿﻿Scientific computing/machine learning (<mark style="color:yellow;">matrix multiply</mark>)
    * ﻿﻿Webserver: handle many <mark style="color:yellow;">unrelated requests simultaneously</mark>,
    * ﻿﻿Input / output: it's all happening simultaneously!
* ﻿﻿Specialized languages for different tasks
  * ﻿﻿Some are easier to use (for some problems)
  * ﻿﻿None is particularly "easy" to use

## <mark style="color:red;">OpenMP</mark>

<figure><img src=".gitbook/assets/image (2) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1).png" alt=""><figcaption></figcaption></figure>

* <mark style="color:yellow;">C extension</mark>: no new language to learn
* ﻿﻿<mark style="color:yellow;">Multi-threaded,</mark> <mark style="color:red;">shared-memory parallelism</mark>
  * ﻿﻿Compiler Directives, <mark style="color:yellow;">#pragma</mark>
  * ﻿﻿Runtime Library Routines, <mark style="color:yellow;">#include \<omp. h></mark>
* ﻿﻿<mark style="color:yellow;">#pragma</mark>
  * ﻿﻿<mark style="color:yellow;">Ignored by compilers unaware of OpenMP</mark>
  * ﻿﻿<mark style="color:yellow;">Same source for multiple architectures</mark>
    * ﻿﻿E.g., same program for 1 & 16 cores
* ﻿﻿<mark style="color:red;">**Only**</mark> <mark style="color:yellow;">works with</mark> <mark style="color:red;">shared memory</mark>

{% hint style="info" %}
OpenMP is designed for <mark style="color:red;">**shared-memory architectures**</mark> because <mark style="color:yellow;">it relies on threads running in the same address space to share data</mark>. In a shared memory model, all threads have direct access to the same memory, enabling efficient communication and synchronization between them using OpenMP's constructs like `#pragma omp parallel`.

In contrast, <mark style="color:red;">**distributed-memory systems**</mark> (e.g., clusters) require explicit data transfer between memory spaces (e.g., using MPI). OpenMP does not natively support this, as it assumes the memory is shared and accessible by all threads without explicit communication.
{% endhint %}

## OpenMP 语法-private和share变量定义

1\. `private` 变量

`private` 变量在<mark style="color:red;">每个线程中都有独立的副本，线程之间不会共享这些变量的值</mark>。每个线程对 `private` 变量的修改不会影响其他线程中的同名变量。

**定义方式：**

* 使用 `private` 子句在并行区域中<mark style="color:red;">声明变量为私有</mark>。

```
#include <omp.h>
#include <stdio.h>

int main() {
    int a = 10;

    #pragma omp parallel private(a)
    {
        printf("Thread %d: a = %d\n", omp_get_thread_num(), a);
        a = omp_get_thread_num();  // 每个线程修改自己的 a
        printf("Thread %d: a = %d\n", omp_get_thread_num(), a);
    }

    printf("After parallel region: a = %d\n", a);  // a 的值仍然是 10
    return 0;
}
```

#### 2. `shared` 变量

`shared` 变量在<mark style="color:red;">所有线程之间共享，所有线程访问的是同一个内存地址</mark>。一个线程对 `shared` 变量的修改会影响其他线程。

**定义方式：**

* 默认情况下，<mark style="color:red;">并行区域外的变量在并行区域内是</mark> <mark style="color:red;"></mark><mark style="color:red;">`shared`</mark> <mark style="color:red;"></mark><mark style="color:red;">的，除非显式声明为</mark> <mark style="color:red;"></mark><mark style="color:red;">`private`</mark>。
* 可以使用 `shared` 子句显式声明变量为共享。

```
#include <omp.h>
#include <stdio.h>

int main() {
    int a = 10;

    #pragma omp parallel shared(a)
    {
        printf("Thread %d: a = %d\n", omp_get_thread_num(), a);
        #pragma omp critical
        {
            a += omp_get_thread_num();  // 所有线程共享 a，使用 critical 避免竞争
        }
        printf("Thread %d: a = %d\n", omp_get_thread_num(), a);
    }

    printf("After parallel region: a = %d\n", a);  // a 的值被所有线程修改
    return 0;
}
```

#### 3. 默认行为

* <mark style="color:red;">在并行区域内，外部变量默认是</mark> <mark style="color:red;"></mark><mark style="color:red;">`shared`</mark> <mark style="color:red;"></mark><mark style="color:red;">的</mark>。
  * mask，ham\_dist默认是shared
  * i是默认是private的
* 循环变量（如 `for` 循环中的索引）在 `parallel for` 中默认是 `private` 的。

```
uint32_t hamming(uint32_t x, uint32_t y) {
    uint32_t mask, ham_dist = 0;
    #pragma omp for
    for (int i = 0; i <= 31; i++) {
        mask = 1 << i;
        if ((y & mask) != (x & mask))
            ham_dist++;
    }
    return ham_dist;
}
```

## OpenMP 语法-宏定义

* \#pragma omp parallel
  * `#pragma omp parallel` 后面的代码块会被分配到所有线程中运行一遍。具体来说，当程序执行到 `#pragma omp parallel` 时，OpenMP 会创建一组线程（线程数量由环境变量或运行时库决定），每个线程都会独立执行 `#pragma omp parallel` 后面的代码块。
  *   示例代码：

      ```
      #include <omp.h>
      #include <stdio.h>

      int main() {
          #pragma omp parallel
          {
              int thread_id = omp_get_thread_num();
              printf("Hello from thread %d\n", thread_id);
          }

          printf("Parallel region ended. Back to main thread.\n");
          return 0;
      }
      ```

      **输出示例：**

      假设有 4 个线程，输出可能如下（顺序可能不同，因为线程是并发执行的）：

      ```
      Hello from thread 0
      Hello from thread 2
      Hello from thread 1
      Hello from thread 3
      Parallel region ended. Back to main thread.
      ```
* \#pragma omp parallel for
  * `#pragma omp parallel for` 是 OpenMP 中用于并行化循环的指令。<mark style="color:red;">它将</mark> <mark style="color:red;"></mark><mark style="color:red;">`#pragma omp parallel`</mark> <mark style="color:red;"></mark><mark style="color:red;">和</mark> <mark style="color:red;"></mark><mark style="color:red;">`#pragma omp for`</mark> <mark style="color:red;"></mark><mark style="color:red;">结合在一起</mark>，表示在并行区域内对紧随其后的 `for` 循环进行并行化。每个线程会分配到循环的一部分迭代，从而加速循环的执行。下面两段代码是等效的
  * `#pragma omp parallel for`自动将loop variable转换成private变量。
    * ```
      //Set all elements in arr to 17
      int A[1048576];
      int i;

      #pragma omp parallel for
      for (i = 0; i < 1048576; i++){
          A[i] = 17;
      }
      ```
    *

        ```
        //Set all elements in arr to 17
        int A[1048576];
        #pragma omp parallel
        {
            #pragma omp for
            for (int i = 0; i < 1048576; i++){
                A[i] = 17;
            }
        }
        ```
    *

        ```
        //Set all elements in arr to 17
        int A[1048576];
        #pragma omp parallel for
        for (int i = 0; i < 1048576; i++){
            A[i] = 17;
        }
        ```

## OpenMP Example

<mark style="color:red;">private\_sum</mark> is a private variable since it is declared inside the <mark style="color:red;">#pragma omp parallel</mark> directive. The <mark style="color:red;">omp for</mark> directive then divides the for loop into the different threads. Each thread will get to update their <mark style="color:red;">private\_sum</mark> variable. The final summation <mark style="color:red;">is done within the critical statement once the loop for all threads is done</mark>.&#x20;

```
int A[1048576];
initializedata(&A);
int sum = 0;
#pragma omp parallel
{
    int private_sum = 0;
    #pragma omp for
    for (int i = 0; i < 1048576; i++){
        private_sum += A[i];
    }
    #pragma omp critical
    sum += private_sum;
}
```



## OpenMP Programming Model

<figure><img src=".gitbook/assets/image (282).png" alt="" width="563"><figcaption></figcaption></figure>

* OpenMP programs begin as single process (main thread)
  * Sequential execution
* When parallel region is encountered
  * Master thread <mark style="color:yellow;">"forks"</mark> into team of parallel threads
  * <mark style="color:yellow;">Executed simultaneously</mark>
  * At end of parallel region, parallel threads <mark style="color:yellow;">"join"</mark>, leaving only master thread
* Process repeats for each parallel region
  * Amdahl's Law?

## What Kind of Threads?

* ﻿﻿OpenMP threads are <mark style="color:red;">operating system (software) threads</mark>
* ﻿﻿<mark style="color:red;">OS will multiplex requested OpenMP threads onto available hardware threads</mark>
* ﻿﻿Hopefully each gets a real hardware thread to run on, so no OS-level time-multiplexing
* ﻿﻿But other tasks on machine compete for hardware threads!
* ﻿﻿Be "careful" (?) when timing results for Projects!
  * ﻿﻿5АМ?
  * ﻿﻿Job queue?

## Computing PI

<figure><img src=".gitbook/assets/image (6) (1) (1) (1) (1) (1) (1).png" alt="" width="563"><figcaption></figcaption></figure>

<figure><img src=".gitbook/assets/image (7) (1) (1) (1) (1) (1) (1).png" alt="" width="563"><figcaption></figcaption></figure>

<figure><img src=".gitbook/assets/image (8) (1) (1) (1) (1).png" alt="" width="563"><figcaption></figcaption></figure>

如果for循环中有一个**公用变量**（即多个线程共享的变量），<mark style="color:red;">可能会导致竞争条件（Race Condition）</mark>，多个线程同时访问和修改同一个共享变量，导致程序行为不确定或结果错误。解决方式为用临时变量记录每次的结果，最后合并为一个变量。



<figure><img src=".gitbook/assets/image (9) (1) (1) (1).png" alt="" width="563"><figcaption></figcaption></figure>



### Synchronization

* Problem:
  * ﻿﻿<mark style="color:yellow;">Limit access to shared resource to 1 actor at a time</mark>
  * ﻿﻿E.g. only 1 person permitted to edit a file at a time
    * ﻿﻿otherwise changes by several people get all mixed up

## <mark style="color:red;">Locks</mark>

* Computers use locks to control access to shared resources
  * Serves purpose of microphone in example
  * Also referred to as "<mark style="color:red;">semaphore</mark>"
* Usually implemented with a variable
  * int lock;
    * 0 for unlock
    * 1 for locked

## Lock Synchronization

<mark style="color:red;">Software level(even at the assembly level) lock cannot solve racing problem</mark>, the codes below still racing while trying to get the lock.

<figure><img src=".gitbook/assets/image (38).png" alt="" width="563"><figcaption></figcaption></figure>

## Conclusion

* OpenMP as simple parallel extension to C
  * Threads level programming with parallel for pragma
  * Similar to C: Small so easy to learn, but not very high level and it's easy to get into trouble
* Race condition - result of program depends on chance
  * Need assembly-level instructions to help with lock synchronization
