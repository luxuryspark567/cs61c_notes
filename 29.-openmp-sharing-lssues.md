# 29. OpenMP, Sharing lssues

## Why So Many Parallel Programming Languages?

* ﻿﻿Why "intrinsics"?
  * ﻿﻿TO Intel: fix your compiler, thanks...
* ﻿﻿It's happening ... but
  * ﻿﻿SIMD features are continually added to compilers (Intel, gcc)
  * Intense area of research
  * ﻿﻿Research progress:
    * ﻿﻿<mark style="color:yellow;">20+ years to translate C into good (fast) assembly</mark>
    * ﻿﻿How long to <mark style="color:yellow;">translate C into good (fast) parallel code</mark>?
      * <mark style="color:yellow;">General problem is very hard to solve</mark>
      * Present state: <mark style="color:yellow;">specialized solutions for specific cases</mark>
      * Your opportunity to become famous!

## Parallel Programming Languages

* ﻿﻿Number of choices is indication of
  * ﻿﻿No universal solution, needs are very problem specific
    * ﻿﻿Scientific computing/machine learning (<mark style="color:yellow;">matrix multiply</mark>)
    * ﻿﻿Webserver: handle many <mark style="color:yellow;">unrelated requests simultaneously</mark>,
    * ﻿﻿Input / output: it's all happening simultaneously!
* ﻿﻿Specialized languages for different tasks
  * ﻿﻿Some are easier to use (for some problems)
  * ﻿﻿None is particularly "easy" to use

## OpenMP

<figure><img src=".gitbook/assets/image (2) (1) (1) (1) (1) (1) (1) (1) (1) (1).png" alt=""><figcaption></figcaption></figure>

* <mark style="color:yellow;">C extension</mark>: no new language to learn
* ﻿﻿<mark style="color:yellow;">Multi-threaded, shared-memory parallelism</mark>
  * ﻿﻿Compiler Directives, <mark style="color:yellow;">#pragma</mark>
  * ﻿﻿Runtime Library Routines, <mark style="color:yellow;">#include \<omp. h></mark>
* ﻿﻿<mark style="color:yellow;">#pragma</mark>
  * ﻿﻿<mark style="color:yellow;">Ignored by compilers unaware of OpenMP</mark>
  * ﻿﻿<mark style="color:yellow;">Same source for multiple architectures</mark>
    * ﻿﻿E.g., same program for 1 & 16 cores
* ﻿﻿<mark style="color:red;">**Only**</mark> <mark style="color:yellow;">works with shared memory</mark>

{% hint style="info" %}
OpenMP is designed for **shared-memory architectures** because <mark style="color:yellow;">it relies on threads running in the same address space to share data</mark>. In a shared memory model, all threads have direct access to the same memory, enabling efficient communication and synchronization between them using OpenMP's constructs like `#pragma omp parallel`.

In contrast, **distributed-memory systems** (e.g., clusters) require explicit data transfer between memory spaces (e.g., using MPI). OpenMP does not natively support this, as it assumes the memory is shared and accessible by all threads without explicit communication.
{% endhint %}

## OpenMP Programming Model

<figure><img src=".gitbook/assets/image (282).png" alt="" width="563"><figcaption></figcaption></figure>

* OpenMP programs begin as single process (main thread)
  * Sequential execution
* When parallel region is encountered
  * Master thread <mark style="color:yellow;">"forks"</mark> into team of parallel threads
  * <mark style="color:yellow;">Executed simultaneously</mark>
  * At end of parallel region, parallel threads <mark style="color:yellow;">"join"</mark>, leaving only master thread
* Process repeats for each parallel region
  * Amdahl's Law?

## What Kind of Threads?

* ﻿﻿OpenMP threads are <mark style="color:yellow;">operating system (software) threads</mark>
* ﻿﻿<mark style="color:yellow;">OS will multiplex requested OpenMP threads onto available hardware threads</mark>
* ﻿﻿Hopefully each gets a real hardware thread to run on, so no OS-level time-multiplexing
* ﻿﻿But other tasks on machine compete for hardware threads!
* ﻿﻿Be "careful" (?) when timing results for Projects!
  * ﻿﻿5АМ?
  * ﻿﻿Job queue?

## Computing PI

<figure><img src=".gitbook/assets/image (6) (1) (1) (1) (1) (1).png" alt="" width="375"><figcaption></figcaption></figure>

<figure><img src=".gitbook/assets/image (7) (1) (1) (1) (1) (1).png" alt="" width="375"><figcaption></figcaption></figure>

<figure><img src=".gitbook/assets/image (8) (1) (1) (1).png" alt="" width="375"><figcaption></figcaption></figure>

<figure><img src=".gitbook/assets/image (9) (1) (1) (1).png" alt="" width="563"><figcaption></figcaption></figure>

## Can we Parallelize Computing sum?

<figure><img src=".gitbook/assets/image (11) (1) (1) (1).png" alt="" width="563"><figcaption></figcaption></figure>

<figure><img src=".gitbook/assets/image (12) (1) (1) (1).png" alt="" width="563"><figcaption></figcaption></figure>



Synchronization

*   Problem:

    * ﻿﻿<mark style="color:yellow;">Limit access to shared resource to 1 actor at a time</mark>
    *   ﻿﻿E.g. only 1 person permitted to edit a file at a time

        * ﻿﻿otherwise changes by several people get all mixed up





## Locks

* Computers use locks to control access to shared resources
  * Serves purpose of microphone in example
  * Also referred to as "semaphore"
* Usually implemented with a variable
  * int lock;
    * 0 for unlock
    * 1 for locked

## Lock Synchronization

Software level(even at the assembly level) lock cannot solve racing problem, the codes below still racing while trying to get the lock.

<figure><img src=".gitbook/assets/image (38).png" alt="" width="563"><figcaption></figcaption></figure>

## Conclusion

* OpenMP as simple parallel extension to C
  * Threads level programming with parallel for pragma
  * Similar to C: Small so easy to learn, but not very high level and it's easy to get into trouble
* Race condition - result of program depends on chance
  * Need assembly-level instructions to help with lock synchronization
