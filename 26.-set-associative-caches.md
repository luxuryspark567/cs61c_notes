# 26. Set-Associative Caches

## N-Way Set Associative Cache

* Memory address fields:&#x20;
  * Tag: same as before&#x20;
  * Offset: same as before&#x20;
  * Index: <mark style="color:yellow;">points us to the correct “row”</mark> (called <mark style="color:yellow;">a set</mark> in this case)&#x20;
* So what’s the difference?&#x20;
  * <mark style="color:yellow;">each set contains multiple blocks</mark>&#x20;
  * <mark style="color:yellow;">once we’ve found correct set, must compare with all tags in that set to find our data</mark>&#x20;
  * Size of $ = # sets \* N blocks/set \* block size

{% hint style="info" %}
相当于多了一个维度，之前是一个index对应一个set，现在是一个index对应多个set。对应2个就是2-Way Set Associative Cache，对应N个就是N-Way Set Associative Cache。
{% endhint %}

## 2-way set associative cache: 2 sets, 2 blocks in set

<figure><img src=".gitbook/assets/image (16) (1) (1) (1).png" alt="" width="563"><figcaption></figcaption></figure>



* Basic Idea&#x20;
  * cache is direct-mapped w/respect to sets&#x20;
  * each set is fully associative with N blocks in it&#x20;
* Given memory address:&#x20;
  * Find correct set using Index value.&#x20;
  * Compare Tag with all Tag values in that set.&#x20;
  * If a match occurs, hit!, otherwise a miss.&#x20;
  * Finally, use the offset field as usual to find the desired data within the block.
* What’s so great about this?&#x20;
  * even a 2-way set assoc cache avoids a lot of conflict misses&#x20;
  * hardware cost isn’t that bad: only need N comparators&#x20;
* In fact, for a cache with M blocks,&#x20;
  * it’s Direct-Mapped if it’s 1-way set assoc&#x20;
  * it’s Fully Assoc if it’s M-way set assoc&#x20;
  * so these two are just special cases of the more general set associative design

## 4-Way Set Associative Cache Circuit

<figure><img src=".gitbook/assets/image (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1).png" alt=""><figcaption></figcaption></figure>

## Block Replacement

* Direct-Mapped Cache&#x20;
  * index completely specifies position which position a block can go in on a miss&#x20;
* N-Way Set Assoc&#x20;
  * index specifies a set, but block can occupy any position within the set on a miss&#x20;
* Fully Associative&#x20;
  * block can be written into any position&#x20;
* Question: if we have the choice, where should we write an incoming block?&#x20;
  * <mark style="color:yellow;">If there’s a valid bit off, write new block into first invalid.</mark>&#x20;
  * <mark style="color:yellow;">If all are valid, pick a replacement policy</mark>&#x20;
    * rule for which block gets “cached out” on a miss.

## Block Replacement Policy

* LRU (Least Recently Used)&#x20;
  * Idea: cache out block which has been accessed (read or write) least recently&#x20;
  * Pro: temporal locality -> recent past use implies likely future use: in fact, this is a very effective policy&#x20;
  * Con: with 2-way set assoc, easy to keep track (one LRU bit); with 4-way or greater, requires complicated hardware and much time to keep track of this&#x20;
* FIFO&#x20;
  * Idea: ignores accesses, just tracks initial order&#x20;
* Random&#x20;
  * If low temporal locality of workload, works ok

## Example

Our same 2-way set associative cache with a four byte total capacity and one byte blocks.  We perform the following byte accesses:

&#x20; 0, 2, 0, 1, 4, 0, 2, 3, 5, 4

How many hits and how many misses will there be for the LRU replacement policy?

<figure><img src=".gitbook/assets/image (2) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1).png" alt=""><figcaption></figcaption></figure>

<figure><img src=".gitbook/assets/image (3) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1).png" alt=""><figcaption></figcaption></figure>

Simulator: www.ecs.umass.edu/ece/koren/architecture/Cache/frame1.htm

## Average Memory Access Time (AMAT)

* Design against a performance mode
  * Minimize: <mark style="color:yellow;">Average Memory Access Time  = Hit Time +  Miss Penalty x Miss Rate</mark>
  * Influenced by technology & program behavior
* Create the illusion of a memory that is large, cheap, and fast - on average

## Improving Miss Penalty

* When caches first became popular, Miss Penalty \~ 10 processor clock cycles&#x20;
* Today 3 GHz Processor (1/3 ns per clock cycle) and 80 ns to go to DRAM (\~200 processor clock cycles!)
* Solution:&#x20;
  * another cache between memory and the processor cache: <mark style="color:yellow;">Second Level (L2) Cache</mark>

<figure><img src=".gitbook/assets/image (4) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1).png" alt="" width="375"><figcaption></figcaption></figure>

## Analyzing Multi-level cache hierarchy

<figure><img src=".gitbook/assets/image (5) (1) (1) (1) (1) (1) (1) (1) (1) (1).png" alt="" width="375"><figcaption></figcaption></figure>

<figure><img src=".gitbook/assets/image (6) (1) (1) (1) (1) (1) (1) (1) (1).png" alt="" width="563"><figcaption></figcaption></figure>

## Example

* Assume
  * Hit Time = 1 cycle
  * Miss rate = 5%
  * Miss penalty = 20 cycles
* AMAT = 1 + 0.05 x 20 = 2 cycles

## Ways to reduce miss rate

* <mark style="color:yellow;">Larger cache</mark>&#x20;
  * limited by cost and technology&#x20;
  * hit time of first level cache < cycle time (bigger caches are slower)&#x20;
* <mark style="color:yellow;">More places in the cache to put each block of memory</mark> – associativity&#x20;
  * fully-associative&#x20;
    * any block any line&#x20;
  * N-way set associated&#x20;
    * N places for each block&#x20;
    * direct map: N=1

## Typical Scale

* L1&#x20;
  * size: tens of KB&#x20;
  * hit time: complete in one clock cycle&#x20;
  * miss rates: 1-5%&#x20;
* L2:&#x20;
  * size: hundreds of KB&#x20;
  * hit time: few clock cycles&#x20;
  * miss rates: 10-20%&#x20;
* L2 miss rate is fraction of L1 misses that also miss in L2&#x20;
  * why so high?

Example:&#x20;

* with L2 cache
  * Assume&#x20;
    * L1 Hit Time = 1 cycle L1&#x20;
    * Miss rate = 5%&#x20;
    * L2 Hit Time = 5 cycles&#x20;
    * L2 Miss rate = 15% (% L1 misses that miss)&#x20;
    * L2 Miss Penalty = 200 cycles&#x20;
  * L1 miss penalty = 5 + 0.15 \* 200 = 35&#x20;
  * Avg mem access time = 1 + 0.05 x 35 = 2.75 cycles
* without L2 cache
  * Assume&#x20;
    * L1 Hit Time = 1 cycle&#x20;
    * L1 Miss rate = 5%&#x20;
    * L1 Miss Penalty = 200 cycles&#x20;
  * Avg mem access time = 1 + 0.05 x 200 = 11 cycles
  * <mark style="color:yellow;">4x faster with L2 cache</mark>! (2.75 vs. 11)

## An Actual CPU – Pentium M

<figure><img src=".gitbook/assets/image (7) (1) (1) (1) (1) (1) (1) (1) (1).png" alt=""><figcaption></figcaption></figure>

## Conclution

* Big idea:&#x20;
  * <mark style="color:yellow;">if something is expensive but we want to do it repeatedly, do it once and cache the result.</mark>&#x20;
* Cache design choices:&#x20;
  * Size of cache: speed v. capacity&#x20;
  * Block size (i.e., cache aspect ratio)&#x20;
  * Write Policy (Write through v. write back&#x20;
  * Associativity choice of N (direct-mapped v. set v. fully associative)&#x20;
  * Block replacement policy&#x20;
  * 2nd level cache?&#x20;
  * 3rd level cache?&#x20;
* Use performance model to pick between choices, depending on programs, technology, budget, ...
