# 30. Cache Conherence, Performance

## Hardware Synchronization

## Review: OpenMP Building Block: for loop

* ﻿﻿OpenMP as simple parallel extension to C
  * ﻿﻿Threads level programming with parallel for pragma&#x20;
  * \~ C: small so easy to learn, but not very high level and it's easy to get into trouble\
    for (1=0; i\<max; i++) zerofil = 0;
* ﻿﻿Breaks for loop into chunks, and allocate each to a separate thread
  * e.g. if max = 100 with 2 threads: assign 0-49 to thread 0, and 50-99 to thread 1
* ﻿﻿Must have relatively simple "shape" for an OpenMP-aware compiler to be able to parallelize it
  * Necessary for the run-time system to be able to determine how many of the loop iterations to assign to each thread
* No premature exits from the loop allowed&#x20;
  * i.e. No break, return, exit, goto statements // In general, don't jump outside of any pragma block

<figure><img src=".gitbook/assets/image (259).png" alt=""><figcaption></figcaption></figure>

## Review: Data Races and Synchronization

* ﻿﻿Two memory accesses form a data race if from different threads access same location, at least one is a write, and they occur one after another
  * ﻿﻿If there is a data race, result of program varies depending on chance (which thread first?)
* ﻿﻿Avoid data races by synchronizing writing and reading to get deterministic behavior
  * ﻿﻿Synchronization done by user-level routines that rely on hardware synchronization instructions

<figure><img src=".gitbook/assets/image (260).png" alt=""><figcaption></figcaption></figure>

## Hardware Synchronization

* ﻿Solution:
  * ﻿﻿Atomic read/write
  * ﻿﻿Read & write in single instruction
    * ﻿﻿No other access permitted between read and write
  * ﻿﻿Note:
    * ﻿﻿Must use shared memory (multiprocessing)
* ﻿﻿Common implementations:
  * ﻿﻿Atomic swap of register - memory
  * ﻿﻿Pair of instructions for "linked" read and write
    * ﻿﻿write fails if memory location has been "tampered" with after linked read
* ﻿﻿RISC-V has variations of both, but for simplicity we will focus on the former

<figure><img src=".gitbook/assets/image (261).png" alt=""><figcaption></figcaption></figure>

## RISC-V Atomic Memory Operations (AMOs)

* ﻿﻿AMOs atomically perform an operation on an operand in memory and set the destination register to the original memory value
* ﻿﻿R-Type Instruction Format: Add, And, Or, Swap, Xor, Max, Max Unsigned, Min, Min Unsigned

<figure><img src=".gitbook/assets/image (262).png" alt=""><figcaption></figcaption></figure>

## RISCV Critical Section

* ﻿﻿Assume that the lock is in memory location stored in register a
* ﻿﻿The lock is "set" if it is 1; it is "free" if it is 0 (its initial value)

<figure><img src=".gitbook/assets/image (263).png" alt=""><figcaption></figcaption></figure>



<figure><img src=".gitbook/assets/image (264).png" alt=""><figcaption></figcaption></figure>



<figure><img src=".gitbook/assets/image (265).png" alt=""><figcaption></figcaption></figure>

## Synchronization in OpenMP

* ﻿﻿Typically are used in libraries of higher-level parallel programming constructs
* ﻿﻿E.g. OpenMP offers #pragmas for common cases:
  * ﻿﻿critical
  * atomic
  * barrier
  * ﻿﻿ordered
* ﻿﻿OpenMP offers many more features
  * E.g., private variables, reductions
  * ﻿﻿See online documentation
  * Or tutorial at
    * ﻿﻿http://openmp.org/mp-documents/omp-hands-on-SC08.pdf

<figure><img src=".gitbook/assets/image (266).png" alt=""><figcaption></figcaption></figure>



<figure><img src=".gitbook/assets/image (267).png" alt=""><figcaption></figcaption></figure>

## Deadlock

* ﻿﻿Deadlock: a system state in which no progress is possible
* ﻿﻿Dining Philosopher's Problem:
  * Think until the left fork is available; when it is, pick it up&#x20;
  * Think until the right fork is available; when it is, pick it up&#x20;
  * When both forks are held, eat for a fixed amount of time&#x20;
  * Then, put the right fork down&#x20;
  * Then, put the left fork down&#x20;
  * Repeat from the beginning

<figure><img src=".gitbook/assets/image (268).png" alt=""><figcaption></figcaption></figure>

## OpenMP Timing

* Elapsed wall clock time:\
  double omp\_get\_wtime (void) ;
  * ﻿﻿Returns elapsed wall clock time in seconds
  * ﻿﻿Time is measured per thread, no guarantee can be made that two distinct threads measure the same time
  * ﻿﻿Time is measured from "some time in the past", so subtract results of two calls to omp\_get\_wtime to get elapsed time

<figure><img src=".gitbook/assets/image (269).png" alt=""><figcaption></figcaption></figure>



## Shared Memory and Caches

* SMP: (Shared Memory) Symmetric Multiprocessor
  * ﻿﻿Two or more identical CPUs/Cores
  * ﻿﻿Single shared coherent memory

<figure><img src=".gitbook/assets/image (270).png" alt=""><figcaption></figcaption></figure>

## Multiprocessor Key Questions

* ﻿﻿QI - How do they share data?
* ﻿﻿Q2 - How do they coordinate?
* ﻿﻿Q3 - How many processors can be supported?

<figure><img src=".gitbook/assets/image (271).png" alt=""><figcaption></figcaption></figure>

## Shared Memory Multiprocessor (SMP)

* ﻿﻿Ql - Single address space shared by all processors/cores
* ﻿﻿Q2 - Processors coordinate/communicate through shared variables in memory (via loads and stores)
  * ﻿﻿Use of shared data must be coordinated via synchronization primitives (locks) that allow access to data to only one processor at a time
* ﻿﻿All multicore computers today are SMP

<figure><img src=".gitbook/assets/image (272).png" alt=""><figcaption></figcaption></figure>

## Multiprocessor Caches

* ﻿﻿Memory is a performance bottleneck even with one processor
* ﻿﻿Use caches to reduce bandwidth demands on main memory
* ﻿﻿Each core has a local private cache holding data it has accessed recently
* ﻿﻿Only cache misses have to access the shared common memory



<figure><img src=".gitbook/assets/image (273).png" alt=""><figcaption></figcaption></figure>



## Cache Coherency

## Keeping Multiple Caches Coherent

* ﻿﻿Architect's job: shared memory → keep cache values coherent
* ﻿﻿Idea: When any processor has cache miss or writes, notify other processors via interconnection network
  * ﻿﻿If only reading, many processors can have copies
  * ﻿﻿If a processor writes, invalidate any other copies
* ﻿﻿Write transactions from one processor, other caches "snoop" the common interconnect checking for tags they hold
  * ﻿﻿Invalidate any copies of same address modified in other cache

<figure><img src=".gitbook/assets/image (274).png" alt=""><figcaption></figcaption></figure>

## How Does HW Keep $ Coherent?

* Each cache tracks state of each block in cache:
  * ﻿﻿﻿Shared: up-to-date data, other caches may have a copy
  * Modified: up-to-date data, changed (dirty), no other cache has a copy, OK to write, memory out-of-date (i.e., write back

<figure><img src=".gitbook/assets/image (275).png" alt=""><figcaption></figcaption></figure>

## Two Optional Performance Optimizations of Cache Coherency via New States

* Each cache tracks state of each block in cache:
* Exclusive: up-to-date data, no other cache has a copy, OK to write, memory up-to-date
  * Avoids writing to memory if block replaced
  * Supplies data on read instead of going to memory
* ﻿﻿﻿Owner: up-to-date data, other caches may have a copy (they must be in Shared state)
  * This cache is one of several with a valid copy of the cache line, but has the exclusive right to make changes to it. It must broadcast those changes to all other caches sharing the line. The introduction of owned state allows dirty sharing of data, i.e., a modified cache block can be moved around various caches without updating main memory. The cache line may be changed to the Modified state after invalidating all shared copies, or changed to the Shared state by writing the modifications back to main memory. Owned cache lines must respond to a snoop request with data.

<figure><img src=".gitbook/assets/image (276).png" alt=""><figcaption></figcaption></figure>





<figure><img src=".gitbook/assets/image (277).png" alt=""><figcaption></figcaption></figure>

## Cache Coherency Tracked by Block

* Suppose block size is 32 bytes
* ﻿﻿Suppose Processor 0 reading and writing variable X, Processor 1 reading and writing variable Y
* ﻿﻿Suppose in X location 4000, Y in 4012
* ﻿﻿What will happen?

<figure><img src=".gitbook/assets/image (278).png" alt=""><figcaption></figcaption></figure>

## Coherency Tracked by Cache Block

* ﻿﻿Block ping-pongs between two caches even though processors are accessing disjoint variables
* ﻿﻿Effect called false sharing
* ﻿﻿How can you prevent it?

<figure><img src=".gitbook/assets/image (279).png" alt=""><figcaption></figcaption></figure>

## Fourth "C" of Cache Misses! Coherence Misses

* ﻿﻿Misses caused by coherence traffic with other processor
* ﻿﻿Also known as communication misses because represents data moving between processors working together on a parallel program
* ﻿﻿For some parallel programs, coherence misses can dominate total misses

<figure><img src=".gitbook/assets/image (280).png" alt=""><figcaption></figcaption></figure>

## And, in Conclusion, ...

* ﻿﻿OpenMP as simple parallel extension to C
  * ﻿﻿Threads level programming with parallel for pragma, private variables, reductions,...
  * ﻿﻿\~ C: small so easy to learn, but not very high level and it's easy to get into trouble
* ﻿﻿TLP
  * ﻿﻿Cache coherency implements shared memory even with multiple copies in multiple caches
  * False sharing a concern; watch block size!

<figure><img src=".gitbook/assets/image (281).png" alt=""><figcaption></figcaption></figure>



