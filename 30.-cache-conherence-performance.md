# 30. Cache Conherence, Performance

## Review: OpenMP Building Block: for loop

* ﻿﻿OpenMP as <mark style="color:yellow;">simple parallel extension to C</mark>
  * ﻿﻿<mark style="color:yellow;">Threads level</mark> programming with parallel for pragma&#x20;
  * \~ C: small so easy to learn, but not very high level and it's easy to get into trouble\
    for (1=0; i\<max; i++) zerofil = 0;
* ﻿﻿<mark style="color:yellow;">Breaks for loop into chunks, and allocate each to a separate thread</mark>
  * e.g. if max = 100 with 2 threads: assign 0-49 to thread 0, and 50-99 to thread 1
* ﻿﻿<mark style="color:yellow;">Must have relatively simple "shape" for an OpenMP-aware compiler to be able to parallelize it</mark>
  * Necessary for the run-time system to be able to determine how many of the loop iterations to assign to each thread
* <mark style="color:yellow;">No premature exits</mark> from the loop allowed&#x20;
  * i.e. <mark style="color:yellow;">No break, return, exit, goto statements</mark> // In general, don't jump outside of any pragma block

## Review: Data Races and Synchronization

* ﻿﻿Two memory accesses form a data race if from <mark style="color:yellow;">different threads access same location</mark>, <mark style="color:yellow;">at least one is a write</mark>, and they occur one after another
  * ﻿﻿If there is a data race, result of program varies depending on chance (which thread first?)
* ﻿﻿Avoid data races by <mark style="color:yellow;">synchronizing writing and reading</mark> to get deterministic behavior
  * ﻿﻿Synchronization done by user-level routines that <mark style="color:yellow;">rely on hardware synchronization instructions</mark>

## Hardware Synchronization

* ﻿Solution:
  * ﻿﻿<mark style="color:yellow;">Atomic read/write,</mark> Read & write in single instruction
    * ﻿﻿<mark style="color:yellow;">No other access permitted between read and write (</mark><mark style="color:red;">HOW???</mark><mark style="color:yellow;">)</mark>
      * <mark style="color:yellow;">read: set the destination register to the original memory value</mark>
      * <mark style="color:yellow;">write: set the operand in memory to some value</mark>
  * ﻿﻿Note:
    * ﻿﻿<mark style="color:yellow;">Must use shared memory</mark> (multiprocessing)
* ﻿﻿Common implementations:
  * 1, ﻿﻿Atomic swap of register - memory
  * ﻿﻿2, Pair of instructions for <mark style="color:yellow;">"linked" read and write</mark>
    * ﻿﻿write fails if memory location has been "tampered" with after linked read<mark style="color:red;">???</mark>
* ﻿﻿<mark style="color:yellow;">RISC-V has variations of both</mark>, but for simplicity we will focus on the former

## RISC-V <mark style="color:yellow;">Atomic Memory Operations (AMOs)</mark>

* ﻿﻿AMOs atomically <mark style="color:yellow;">perform an operation on an operand in memory and set the destination register to the original memory value</mark>
* ﻿﻿R-Type Instruction Format: Add, And, Or, Swap, Xor, Max, Max Unsigned, Min, Min Unsigned

## RISCV Critical Section

* ﻿﻿Assume that <mark style="color:yellow;">the lock is in memory location</mark> stored in <mark style="color:yellow;">register a0</mark>
* ﻿﻿The <mark style="color:yellow;">lock is "set" if it is 1; it is "free" if it is 0</mark> (its initial value)

<figure><img src=".gitbook/assets/image (1) (1) (1) (1) (1) (1) (1) (1) (1).png" alt=""><figcaption></figcaption></figure>

## Lock Synchronization

* Broken Synchronization in assembly: Load memory is slow and it is highly chance that memory a0 has already been updated after load command and before bne command.
* amoswap is to set memory a0 to t0 and save old value to t1 <mark style="color:yellow;">in just one command (try to get the lock in just one command, this command needs AMO hardware to be implemented)</mark>, whitch is the whole get lock operation, the next bnez is just checking if the racing for lock is suxcessful or not.

```
Loop: 
    lw t0, 0(a0)         #load memory
    bne t0, x0, Loop

    lw t1, 1
    sw t1, 0(a0)

Locked:

Unlock:
    sw x0, 0(a0)
```



<figure><img src=".gitbook/assets/image (1) (1) (1) (1) (1) (1) (1) (1) (1) (1).png" alt=""><figcaption></figcaption></figure>

## OpenMP Locks

<figure><img src=".gitbook/assets/image (2) (1) (1) (1) (1) (1) (1) (1) (1).png" alt="" width="375"><figcaption></figcaption></figure>

## Synchronization in OpenMP

* ﻿﻿Typically are used in libraries of higher-level parallel programming constructs
* ﻿﻿E.g. OpenMP offers #pragmas for common cases:
  * ﻿﻿critical
  * atomic
  * barrier
  * ﻿﻿ordered
* ﻿﻿OpenMP offers many more features
  * E.g., private variables, reductions
  * ﻿﻿See online documentation
  * Or tutorial at
    * ﻿﻿http://openmp.org/mp-documents/omp-hands-on-SC08.pdf

## OpenMP Critical Section

<figure><img src=".gitbook/assets/image (267).png" alt=""><figcaption></figcaption></figure>

## Deadlock

* ﻿﻿Deadlock: <mark style="color:yellow;">a system state in which no progress is possible</mark>
* ﻿﻿Example:
  *   Imagine two processes in a system:

      * **Process 1** holds **Resource A** and waits for **Resource B**.
      * **Process 2** holds **Resource B** and waits for **Resource A**.

      Neither process can continue because both are waiting on the other to release the resource it needs, resulting in a deadlock.

{% hint style="info" %}
#### **Deadlock Example in a Multi-threading Environment**:

Imagine two threads in a program, **Thread 1** and **Thread 2**:

* **Thread 1** locks **Mutex A** and then tries to lock **Mutex B**.
* **Thread 2** locks **Mutex B** and then tries to lock **Mutex A**.

In this case, both threads are in a circular wait, causing a deadlock because neither can proceed until the other releases a mutex.

#### **How to Handle Deadlock**:

1. **Deadlock Prevention**: Modify the system to eliminate one of the Coffman conditions. For example:
   * **Mutual Exclusion**: Make resources sharable, if possible (e.g., allow read-only access to resources).
   * **Hold and Wait**: Require processes to request all resources at once or release held resources before requesting others.
   * **No Preemption**: Allow the system to forcibly take resources away from processes (e.g., rollback or restart processes).
   * **Circular Wait**: Impose an ordering on resources and require processes to request resources in a particular order to prevent circular dependencies.
2. **Deadlock Avoidance**: Use algorithms like the **Banker's Algorithm** to ensure that the system never enters a state where deadlock could occur. This involves preemptively checking if granting a resource request will lead to a potential deadlock state.
3. **Deadlock Detection**: Allow the system to enter a deadlock state but periodically check for deadlocks. If detected, recovery mechanisms are used to break the deadlock, such as terminating a process or rolling back to a safe state.
4. **Deadlock Recovery**: After detecting a deadlock, recovery strategies include:
   * **Process Termination**: Abort one or more processes involved in the deadlock.
   * **Resource Preemption**: Forcefully take resources away from processes involved in the deadlock and reassign them to other processes.

#### **Deadlock in Operating Systems**:

In operating systems, deadlock can happen when processes compete for limited resources like memory, CPU time, or I/O devices. Deadlock detection and resolution techniques in operating systems often involve process scheduling, resource management, and transaction handling.
{% endhint %}

## OpenMP Timing

* Elapsed wall clock time:\
  <mark style="color:yellow;">double omp\_get\_wtime (void) ;</mark>
  * ﻿﻿Returns elapsed <mark style="color:yellow;">wall clock time</mark> in seconds
  * ﻿﻿Time is measured per thread, no guarantee can be made that two distinct threads measure the same time
  * ﻿﻿Time is measured from "some time in the past", so subtract results of two calls to omp\_get\_wtime to get elapsed time

## Shared Memory and Caches

## (Chip) Multicore Multiprocessor

<figure><img src=".gitbook/assets/image (3) (1) (1) (1) (1) (1) (1) (1) (1).png" alt="" width="563"><figcaption></figcaption></figure>

* <mark style="color:yellow;">SMP: (Shared Memory) Symmetric Multiprocessor</mark>
  * ﻿﻿Two or more identical CPUs/Cores
  * ﻿﻿Single shared coherent memory

## Multiprocessor Key Questions

Shared Memory Multiprocessor (SMP)

* ﻿﻿QI - How do they share data?
  * <mark style="color:yellow;">Single address space</mark> <mark style="color:red;">shared by all processors/cores</mark>
* ﻿﻿Q2 - How do they coordinate?
  * Processors coordinate/communicate <mark style="color:yellow;">through shared variables in memory</mark> (<mark style="color:yellow;">via loads and stores</mark>). Use of shared data <mark style="color:red;">must</mark> be coordinated <mark style="color:yellow;">via synchronization primitives (locks) that allow access to data to only one processor at a time</mark>
* ﻿﻿Q3 - How many processors can be supported?
  * <mark style="color:red;">Should be no limits?</mark>. All multicore computers today are SMP

## Multiprocessor Caches

* ﻿﻿Memory is a performance bottleneck even with one processor
* ﻿﻿Use caches to reduce bandwidth demands on main memory
* ﻿﻿Each core <mark style="color:yellow;">has a local private cache</mark> holding data it has accessed recently
* ﻿﻿<mark style="color:yellow;">**Only cache misses**</mark> <mark style="color:yellow;"></mark><mark style="color:yellow;">have to access the shared common memory</mark>

<figure><img src=".gitbook/assets/image (4) (1) (1) (1) (1) (1) (1) (1).png" alt="" width="563"><figcaption></figcaption></figure>

## Cache Coherency

## Keeping Multiple Caches Coherent

* ﻿﻿Architect's job: shared memory → keep cache values coherent
* ﻿﻿Idea: <mark style="color:yellow;">When any processor has cache miss or writes, notify other processors</mark> <mark style="color:red;">via interconnection network</mark>
  * ﻿﻿<mark style="color:yellow;">If only reading, many processors can have copies</mark>
  * ﻿﻿<mark style="color:yellow;">If a processor writes, invalidate any other copies</mark>
* ﻿﻿<mark style="color:yellow;">Write transactions from one processor, other caches "snoop" the common interconnect checking for tags they hold</mark>
  * ﻿﻿<mark style="color:yellow;">Invalidate any copies of same address modified in other cache</mark>

## How Does HW Keep Cache($) Coherent? Two Optional Performance Optimizations of Cache Coherency via New States

Each cache tracks state of each block in cache:

1. <mark style="color:yellow;">Invalid:</mark> The cache does not hold a valid copy of the data.
2. ﻿﻿﻿<mark style="color:yellow;">Shared</mark>: up-to-date data, other caches may have a copy
3. <mark style="color:yellow;">Modified</mark>: up-to-date data, changed (dirty), <mark style="color:red;">**no**</mark> <mark style="color:red;"></mark><mark style="color:red;">other</mark> cache has a copy, OK to write, memory out-of-date (i.e., write back)
4. <mark style="color:yellow;">Exclusive</mark>: up-to-date data, <mark style="color:red;">**no**</mark> <mark style="color:red;"></mark><mark style="color:red;">other</mark> cache has a copy, OK to write, memory up-to-date
   * Avoids writing to memory if block replaced
   * Supplies data on read instead of going to memory (<mark style="color:red;">Other cores can get copy from this cache? No, just this core has the copy, and it is why no teed to go to memory</mark>)
5. ﻿﻿﻿<mark style="color:yellow;">Owner</mark>: up-to-date data, <mark style="color:yellow;">other caches may have a copy (they must be in shared state)</mark>
   * This cache is one of several with a valid copy of the cache line, but <mark style="color:yellow;">has the exclusive right to make changes to it</mark>. It must <mark style="color:yellow;">**broadcast those changes**</mark>**&#x20;to all other caches** sharing the line. The introduction of owned state allows <mark style="color:red;">**dirty sharing of data**</mark>, i.e., a modified cache block can be moved around various caches without updating main memory. The cache line may be :
     * changed to <mark style="color:yellow;">the Modified state after invalidating all shared copies</mark>,&#x20;
     * or changed to <mark style="color:yellow;">the Shared state by writing the modifications back to main memory</mark>.&#x20;
   * Owned cache lines must respond to a <mark style="color:yellow;">snoop request with data</mark>. ( <mark style="color:red;">What does this mean?</mark>)

{% hint style="info" %}
#### **Snooping**:

In a system using a **snooping protocol**, caches monitor the system's bus or memory traffic to detect whether other processors are reading from or writing to the memory addresses that the cache holds. If a processor tries to read or write a cache line that another processor is modifying, it may need to update or invalidate its copy.

* **Snoop Request**: A snoop request is when a processor checks the state of a cache line, typically to see if it holds a valid copy of the requested data.

#### **"Owned Cache Lines Must Respond to a Snoop Request with Data"**:

When a cache line is in the **Owned** state, it means the processor has the most up-to-date version of the data. If another processor sends a snoop request (e.g., a read or a write request) for that memory location, the processor holding the owned cache line must respond by providing the **current data**.

This is important because the **Owned** cache is the authoritative source of the data, and in the case of a snoop request (typically a read request), the cache must supply the data to ensure the system remains coherent and consistent.

#### **Example**:

* **Processor 1** writes to a memory address. The cache holding this address enters the **Owned** state, meaning it has the most up-to-date copy of the data.
* **Processor 2** requests the same memory address (via a snoop). Since the data is owned by Processor 1, Processor 1 must respond with the data to ensure Processor 2 receives the most recent value.
* **Processor 1** ensures that other processors' caches can read the data, maintaining cache coherence.

#### **Why This Matters**:

In a multiprocessor or multicore system, maintaining consistency across caches is critical for correctness and performance. Ensuring that an "Owned" cache line responds to snoop requests guarantees that no processor is working with stale data, which could lead to incorrect computations or data corruption.
{% endhint %}

## Common Cache Coherency Protocol: MOESI

* If one cache is in M, the other could only be in I
* If one cache is in O, the other could be in S or I
* If one cache is in E, the other could be in I
* If one cache is in S, the other could be in O, S or I
* If one cache is in I, the other could be in any states.



<figure><img src=".gitbook/assets/image (5) (1) (1) (1) (1) (1).png" alt="" width="256"><figcaption></figcaption></figure>

Memory access to cach is either

* <mark style="color:blue;">M</mark>odified (in cache)
* <mark style="color:blue;">O</mark>wned (in cache)
* <mark style="color:blue;">E</mark>xclusive (in cache)
* <mark style="color:blue;">S</mark>hared  (in cache)
* <mark style="color:blue;">I</mark>nvalid(in cache)

## Cache Coherency Tracked by Block

<figure><img src=".gitbook/assets/image (6) (1) (1) (1) (1).png" alt="" width="563"><figcaption></figcaption></figure>

* Suppose block size is 32 bytes
* ﻿﻿Suppose Processor 0 reading and writing variable X, Processor 1 reading and writing variable Y
* ﻿﻿Suppose in X location 4000, Y in 4012
* ﻿﻿What will happen?
  * False Sharing: ﻿﻿Block ping-pongs between two caches even though processors are accessing disjoint variables
* ﻿﻿<mark style="color:red;">How can you prevent it?</mark>

## Coherence Misses - Fourth "C" of Cache Misses

* ﻿﻿Misses caused by coherence traffic with other processor
* ﻿﻿Also known as communication misses because represents data moving between processors working together on a parallel program
* ﻿﻿For some parallel programs, coherence misses can dominate total misses

## Conclusion

* ﻿﻿OpenMP as simple parallel extension to C
  * ﻿﻿Threads level programming with parallel for pragma, private variables, reductions,...
  * ﻿﻿\~ C: small so easy to learn, but not very high level and it's easy to get into trouble
* ﻿﻿TLP
  * ﻿﻿Cache coherency implements shared memory even with multiple copies in multiple caches
  * False sharing a concern; watch block size!
