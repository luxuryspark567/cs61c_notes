# 32. Distributed Computing and MapReduce (???)

* <mark style="color:red;">Concurrency</mark> is hard.
  * Separate programs usually don't share memory.
  * Sharing state between different programs is hard.
  * Hard to use locks.
* <mark style="color:red;">Handling failure</mark> is hard.
  * Single program: If a thread crashes, the whole program crashes.
  * Distributed computing: If one program crashes, the others keep going.
  * Can create "zombie processes" that keep running after everyone else is finished.\
    Consumes resources until we restart the system. <mark style="color:red;">(???)</mark>
* <mark style="color:red;">Communication</mark> is hard.
* Programs coordinate by <mark style="color:red;">sending messages</mark> between each other.
* Messages can take time to transmit (e.g. over the Internet).
* It takes time to start communication, transmit bytes of data, access memory, etc.
* What if Program A sends a message to Program B...but Program B isn't expecting to receive a message?
* **Goal**: <mark style="color:red;">Split problem into independent sub-tasks, and minimize communication between programs</mark>.

## <mark style="color:red;">Manager-Worker Framework</mark>

Manager pseudocode:

1. Setup.
2. While there's still work to do:
   1. Wait for a worker to request more work.
   2. Find the next task to do.
   3. Tell that worker what task to do.
3. Repeat once per worker:
   1. Wait for a worker to request more work.
   2. Tell that worker we're all done.
4. Teardown.

Worker pseudocode:

1. Setup.
2. done=False.
3. While done==False:
   1. Send manager a request for work.
   2. Receive message from manager.
   3. If message is work: Do the work.
   4. If message is "all done": Set done=True.
4. Teardown.
